{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -rf runs/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "import cProfile\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.utils as vutils\n",
    "from tensorboardX import SummaryWriter\n",
    "import adabound\n",
    "\n",
    "import mython\n",
    "import styletransfer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_root_dir, transforms=None):\n",
    "        self.image_paths = list(image_root_dir.glob(\"*.jpg\"))\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def expand(self, scale):\n",
    "        self.image_paths *= scale\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = Image.open(str(self.image_paths[index])).convert(\"RGB\")\n",
    "        if self.transforms:\n",
    "            x = self.transforms(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = styletransfer.net.VGGEncoder()\n",
    "decoder = styletransfer.net.VGGDecoder()\n",
    "net = styletransfer.net.Net(encoder, decoder)\n",
    "default_lr = 1e-4\n",
    "lr_decay = 1e-5\n",
    "optimizer = torch.optim.Adam(net.decoder.parameters(), lr=default_lr)\n",
    "# scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "#     optimizer, styletransfer.lr_scheduler.get_scheduler())\n",
    "batch_size = 8\n",
    "max_itr = 160000 // batch_size * 8\n",
    "model_save_interval = 1000\n",
    "style_loss_weight = 10.0\n",
    "logdir = \"runs/{}_styletransfer\".format(datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(512),\n",
    "    torchvision.transforms.RandomCrop(256),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "#content_dataset = torchvision.datasets.FashionMNIST(root=\"~/dataset/fashionmnist\", train=True, download=True, transform=transform)\n",
    "#content_dataset = torchvision.datasets.MNIST(root='/content/mnist', train=True, download=True, transform=transform)\n",
    "content_dataset = ImageDataset(Path.home() / \"dataset/COCO/train2014/Resized512Color\", transform)\n",
    "content_loader = iter(DataLoader(\n",
    "    dataset=content_dataset, batch_size=batch_size, drop_last=True, num_workers=2,\n",
    "    sampler=styletransfer.sampler.InfiniteSamplerWrapper(content_dataset)\n",
    "))\n",
    "style_dataset = ImageDataset(Path.home() / \"dataset/AbstractGallary\", transform)\n",
    "style_dataset.expand(len(content_dataset) // len(style_dataset))\n",
    "style_loader = iter(DataLoader(\n",
    "    dataset=style_dataset, batch_size=batch_size, drop_last=True, num_workers=2,\n",
    "    sampler=styletransfer.sampler.InfiniteSamplerWrapper(style_dataset)\n",
    "))\n",
    "\n",
    "n_epochs = 10\n",
    "es_patience = 3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_output_dir = \"../weights/\"\n",
    "\n",
    "#net.load_state_dict(torch.load(\"../weights/model_bestloss.pth\"))\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_image(writer, iteration, inputs, name, add_hist=False):\n",
    "    if inputs is None:\n",
    "        return\n",
    "    image = vutils.make_grid(inputs)\n",
    "    writer.add_image(\"image/{}\".format(name), image, iteration)\n",
    "    if not add_hist:\n",
    "        return\n",
    "    \n",
    "    for idx in range(inputs.shape[0]):\n",
    "        colors = inputs[idx].detach().cpu().numpy()\n",
    "        colors = colors.reshape(3, -1)\n",
    "        #import pdb; pdb.set_trace()\n",
    "        writer.add_histogram(\"hist/{}{}_red\".format(name, idx), colors[0], iteration)\n",
    "        writer.add_histogram(\"hist/{}{}_green\".format(name, idx), colors[1], iteration)\n",
    "        writer.add_histogram(\"hist/{}{}_blue\".format(name, idx), colors[2], iteration)    \n",
    "\n",
    "def add_summary(writer, iteration, contents=None, styles=None, outputs=None):\n",
    "    add_image(writer, iteration, contents, \"content\", True)\n",
    "    add_image(writer, iteration, styles, \"style\", True)\n",
    "    add_image(writer, iteration, outputs, \"output\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_transfer(encoder, decoder, contents, styles):\n",
    "    net.eval()\n",
    "    adain = styletransfer.function.adaptive_instance_normalization\n",
    "    content_feats = encoder(contents)\n",
    "    style_feats = encoder(styles)\n",
    "    trans_feats = adain(content_feats, style_feats)\n",
    "    output = decoder(trans_feats)\n",
    "    return output\n",
    "\n",
    "def inv_norm(inputs):\n",
    "    inv_normalize = torchvision.transforms.Normalize(\n",
    "        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.255],\n",
    "        std=[1/0.229, 1/0.224, 1/0.255]\n",
    "    )\n",
    "    for i in range(inputs.shape[0]):\n",
    "        inputs[i] = inv_normalize(inputs[i])\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, iteration_count):\n",
    "    \"\"\"Imitating the original implementation\"\"\"\n",
    "    lr = default_lr / (1.0 + lr_decay * iteration_count)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(logdir)\n",
    "for i in range(max_itr):\n",
    "    net.train()\n",
    "    #scheduler.step()\n",
    "    adjust_learning_rate(optimizer, iteration_count=i)\n",
    "    \n",
    "    contents = next(content_loader)\n",
    "    styles = next(style_loader)\n",
    "    contents = contents.to(device=device, dtype=torch.float32)\n",
    "    styles = styles.to(device=device, dtype=torch.float32)\n",
    "    loss_c, loss_s = net(contents, styles)\n",
    "    loss_s = style_loss_weight * loss_s\n",
    "    loss = loss_c + loss_s\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    with torch.autograd.set_detect_anomaly(True):\n",
    "        loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    writer.add_scalar(\"loss_content\", loss_c.item(), i + 1)\n",
    "    writer.add_scalar(\"loss_style\", loss_s.item(), i + 1)\n",
    "    writer.add_scalar(\"loss_total\", loss.item(), i + 1)\n",
    "    if (i + 1) % 200 == 0:\n",
    "        with torch.no_grad():\n",
    "            outputs = style_transfer(encoder, decoder, contents, styles)\n",
    "        add_summary(writer, i + 1, \n",
    "                    inv_norm(contents), inv_norm(styles), inv_norm(outputs))\n",
    "        #add_summary(writer, g_iteration, contents=contents, outputs=output)\n",
    "        \n",
    "    if (i + 1) % model_save_interval == 0:\n",
    "        model_output_path = \"model_decoder_{:08d}itr.pth\".format(i + 1)\n",
    "        torch.save(net.decoder, os.path.join(model_output_dir, model_output_path))\n",
    "    print(\"\\r{} / {} : loss = {:.5f} (= {:.5f} + {:.5f})\".format(\n",
    "            i + 1, max_itr, loss.item(), loss_c.item(), loss_s.item()), end=\"\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_fname = \"/home/kitamura/dataset/COCO/train2014/Original/COCO_train2014_000000000009.jpg\"\n",
    "#style_fname = \"/home/kitamura/dataset/COCO/train2014/Original/COCO_train2014_000000000009.jpg\"\n",
    "style_fname = \"/home/kitamura/dataset/AbstractGallary/Abstract_image_1030.jpg\"\n",
    "\n",
    "trans = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "content = trans(Image.open(content_fname))\n",
    "style = trans(Image.open(style_fname))\n",
    "content = content.to(device).unsqueeze(0)\n",
    "style = style.to(device).unsqueeze(0)\n",
    "network = Network(encoder, decoder)\n",
    "\n",
    "network.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    #output = style_transfer(encoder, decoder, content, style)\n",
    "    #output = inv_norm(output)\n",
    "    output = network(content, style)\n",
    "\n",
    "output = output.detach().cpu().numpy()[0]\n",
    "plt.imshow((output.transpose(1, 2, 0) * 255).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(network.to(\"cpu\"), os.path.join(model_output_dir, \"module.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, encoder, decoder, alpha=1.0):\n",
    "        super(Network, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.alpha = alpha\n",
    "        self.adain = torch.jit.script(styletransfer.function.adaptive_instance_normalization)\n",
    "        self.mean = torch.Tensor([-0.485 / 0.229, -0.456 / 0.224, -0.406 / 0.255]).view(3, 1, 1)\n",
    "        self.std = torch.Tensor([1 / 0.229, 1 / 0.224, 1 / 0.255]).view(3, 1, 1)\n",
    "        \n",
    "    def inv_norm(self, input):\n",
    "        return (input - self.mean) / self.std\n",
    "        \n",
    "    def forward(self, contents, styles):\n",
    "        content_feat = self.encoder(contents)\n",
    "        style_feat = self.encoder(styles)\n",
    "        transfered = (1.0 - self.alpha) * content_feat + self.alpha * self.adain(content_feat, style_feat)\n",
    "        outputs = self.decoder(transfered)\n",
    "        for i in range(outputs.shape[0]):\n",
    "            outputs[i] = self.inv_norm(outputs[i])\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "encoder = styletransfer.net.VGGEncoder()\n",
    "decoder = torch.load(\"../weights/model_decoder_00160000itr.pth\")\n",
    "decoder = decoder.to(device)\n",
    "model = Network(encoder, decoder)\n",
    "sm = torch.jit.script(model)\n",
    "#sm = sm.to(device)\n",
    "\n",
    "def get_image_tensor(fname, device):\n",
    "    trans = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    tensor = trans(Image.open(fname)).unsqueeze(0)\n",
    "    return tensor.to(device)\n",
    "\n",
    "content_fname = \"/home/kitamura/dataset/COCO/train2014/Original/COCO_train2014_000000000009.jpg\"\n",
    "style_fname = \"/home/kitamura/dataset/AbstractGallary/Abstract_image_1030.jpg\"\n",
    "content = get_image_tensor(content_fname, device)\n",
    "style = get_image_tensor(style_fname, device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = mython.debug.start_pdb(\n",
    "        lambda: sm(content, style)\n",
    "    )\n",
    "\n",
    "plt.imshow((output[0].detach().cpu().numpy().transpose(1, 2, 0)  * 255).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.save(\"scripted_network.pt\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
