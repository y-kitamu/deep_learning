{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaIN(nn.Module):\n",
    "    def forward(self, contents, styles):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            contents (torch.Tensor) : 4-dimensional tensor (N, C, H, W)\n",
    "            styles (torch.Tensor)   : 4-dimensional tensor (N, C, H, W)\n",
    "        \"\"\"\n",
    "        c_mean = contents.mean(dim=(2, 3), keepdim=True)\n",
    "        c_var = contents.var(dim=(2, 3), keepdim=True)\n",
    "        s_mean = styles.mean(dim=(2, 3), keepdim=True)\n",
    "        s_var = styles.var(dim=(2, 3), keepdim=True)\n",
    "        return s_var * (contents - c_mean) / (c_var + 1.0e-5) + s_mean\n",
    "\n",
    "    \n",
    "class VGGEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg19', pretrained=True)\n",
    "        self.features = model.features[:21]\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        outputs = []\n",
    "        x = self.features[1](self.features[0](inputs))\n",
    "        outputs.append(x)\n",
    "        x = self.features[3](self.features[2](x))\n",
    "        x = self.features[4](x)\n",
    "        x = self.features[6](self.features[5](x))\n",
    "        outputs.append(x)\n",
    "        x = self.features[8](self.features[7](x))\n",
    "        x = self.features[9](x)\n",
    "        x = self.features[11](self.features[10](x))\n",
    "        outputs.append(x)\n",
    "        x = self.features[13](self.features[12](x))\n",
    "        x = self.features[15](self.features[14](x))\n",
    "        x = self.features[17](self.features[16](x))\n",
    "        x = self.features[18](x)\n",
    "        x = self.features[20](self.features[19](x))\n",
    "        outputs.append(x)\n",
    "        return outputs\n",
    "    \n",
    "class VGGDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=\"reflect\"),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=(2, 2), mode=\"nearest\"),\n",
    "            nn.Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=\"reflect\"),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=\"reflect\"),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=\"reflect\"),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=\"reflect\"),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=(2, 2), mode=\"nearest\"),\n",
    "            nn.Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1,1), padding_mode=\"reflect\"),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=\"reflect\"),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=(2, 2), mode=\"nearest\"),\n",
    "            nn.Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=\"reflect\"),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=\"reflect\"),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)\n",
    "    \n",
    "class AdaINNetwork(nn.Module):\n",
    "    def __init__(self, encoder=VGGEncoder(), decoder=VGGDecoder()):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.adain = AdaIN()\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, contents, styles):\n",
    "        c_features = self.encoder(contents)\n",
    "        s_features = self.encoder(styles)\n",
    "        adain_feat = self.adain(c_features[-1], s_features[-1])\n",
    "        output = self.decoder(adain_feat)\n",
    "        return output, adain_feat, c_features, s_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleLoss(nn.Module):\n",
    "    def forward(self, target_features, adain_feature, style_features):\n",
    "        total_loss = torch.dist(adain_feature, target_features[-1], 2)\n",
    "        for t_feat, s_feat in zip(target_features, style_features):\n",
    "            mean_loss = torch.dist(t_feat.mean(dim=(2, 3)), s_feat.mean(dim=(2, 3)), 2)\n",
    "            var_loss = torch.dist(t_feat.var(dim=(2, 3)), s_feat.var(dim=(2, 3)), 2)\n",
    "            total_loss = total_loss + mean_loss + var_loss\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_root_dir, transforms=None):\n",
    "        self.image_paths = list(image_root_dir.glob(\"*.jpg\"))\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def expand(self, scale):\n",
    "        self.image_paths *= scale\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = Image.open(str(self.image_paths[index]))\n",
    "\n",
    "        if self.transforms:\n",
    "            x = self.transforms(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = VGGEncoder()\n",
    "net = AdaINNetwork()\n",
    "optimizer = torch.optim.Adam(net.decoder.parameters(), lr=0.001)\n",
    "criterion = StyleLoss()\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.RandomVerticalFlip(),\n",
    "    torchvision.transforms.RandomCrop((256, 256)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "batch_size = 8\n",
    "content_dataset = ImageDataset(Path.home() / \"dataset/COCO/train2014/Resized512Color\", transform)\n",
    "style_dataset = ImageDataset(Path.home() / \"dataset/ArtWiki/Resized512\", transform)\n",
    "style_dataset.expand(math.ceil(len(content_dataset) / len(style_dataset)))\n",
    "content_loader = DataLoader(\n",
    "    dataset=content_dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=2)\n",
    "style_loader = DataLoader(\n",
    "    dataset=style_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "n_epochs = 10\n",
    "es_patience = 3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = es_patience\n",
    "for epoch in range(n_epochs):\n",
    "    start_time = time.time()\n",
    "    epoch_loss = 0.0\n",
    "    iteration = 0\n",
    "    for contents, styles in zip(content_loader, style_loader):\n",
    "        # assert contents.shape == (8, 3, 256, 256)\n",
    "        # assert styles.shape == (8, 3, 256, 256)\n",
    "        contents = contents.to(device=device, dtype=torch.float32)\n",
    "        styles = styles.to(device=device, dtype=torch.float32)\n",
    "        optimizer.zero_grad()\n",
    "        output, adain_feat, c_features, s_features = net(contents, styles)\n",
    "        # assert output.shape == (8, 3, 256, 256)\n",
    "        target_features = encoder(output)\n",
    "        # assert len(target_features) == len(s_features)\n",
    "        # assert target_features[-1].shape == s_features[-1].shape\n",
    "        loss = criterion(target_features, adain_feat, s_features)\n",
    "        with torch.autograd.set_detect_anomaly(True):\n",
    "            loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        iteration += 1\n",
    "        print(\"\\rloss = {:.3f}\".format(epoch_loss / iteration), end=\"\")\n",
    "\n",
    "    model_output_path = \"model_{}epoch.pth\".format(epoch + 1)\n",
    "    torch.save(net, model_output_path)\n",
    "    epoch_loss /= iteration\n",
    "    print(\"\\nFinish Epoch {} / {}, Loss = {}\".format(epoch + 1, n_epochs, epoch_loss))\n",
    "    \n",
    "    if epoch == 0:\n",
    "        best_loss = epoch_loss\n",
    "    if epoch_loss - 1.0e-5 < best_loss:\n",
    "        torch.save(net, \"model_bestloss.pth\")\n",
    "        patience = es_patience\n",
    "        best_loss = epoch_loss\n",
    "    else:\n",
    "        patience -= 1\n",
    "        if patience == 0:\n",
    "            print(\"Eary Stopping at Epoch {}\".format(epoch + 1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
